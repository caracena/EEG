% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{hyperref}
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{BrainKDD}{'15 Sydney, Australia}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%\title{Benchmarking for emotion recognition in DEAP dataset}
\title{Towards An Unified Analysis Framework for EEG-based Emotion Recognition}

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
some people
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.


% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
\vspace{6cm}
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\keywords{EEG, Machine Learning, Emotion Recognition}

\section{Introduction}
Emotion is critical aspect of the human behavior that plays 
a  significant role in  activities such as communication and 
learning \cite{}.  Historically, researchers have focused on psycho 
physiological variables, such as posture, language 
(voice intonation), facial expression and gestures to identify 
and classify emotions and relate them to behavior patterns 
 or decision making processes \cite{}.
The use of electroencephalography (EEG) to study the 
emotion recognition  has allowed the 
support of disciplines such as Human Computer Interaction 
(HCI) and Brain Computer Interfaces (BCI) \cite{}. In this context, 
the vision proposed is that in the future, machines should 
have the ability to identify the emotional state of humans 
in order to provide an effective assistance. This could 
represent a relevant advance in fields such as health care 
and education \cite{}.

EEG-based emotion recognition usually follows a sequence 
of steps, such as  stimuli selection, feature selection and 
engineering, classification and evaluation \cite{}. Literature 
reports a high level of heterogeneity on each of these steps, 
given the inherent richness of the data (usually captured through 
a sophisticated configuration of electrodes), the multimodality 
of the stimuli and the wide range of analysis tools. Although 
this could be seen as a beneficial element, the recent increment
 in reported approaches makes it difficult to perform a reliable 
comparison to assess the real impact, applicability  and level 
of generalization that the current studies.  Our vision is that, 
based on the collaborative work with other members of the 
community, in the future it will be possible to have a solid 
benchmark that could increase the performance and pace 
of the research in this area, similarly as benchmarks such 
as  \cite{ILSVRC15} and \cite{Judd_2012} have boosted 
the progress in the areas of image recognition and visual 
saliency, respectively. 

In this paper, our  goal is to firstly  study the quality of 
the available datasets for EEG-based recognition, in 
terms of their level of generalization. After choosing 
one, select the current state of the art  in terms of feature
 extraction and try to replicate the their results keeping  
 in mind the unification as a primary goal. Additionally, a 
 evaluation framework is proposed, in order to formalize 
 and normalize the results.  
 
After setting up a baseline based on the study of related 
work, we explored the use of a new paradigm  based on 
Representation Learning: the use of Convolutional Neural 
Networks (CNN) for  EEG-based emotion recognition.  Our main
motivation is try to generate an alternative to the feature 
engineering process present in EEG-based emotion recognition, 
which usually turns out to be difficult and  excessively laborious. In 
that sense, we are interested in analyzing if an automatic feature 
extraction could be competitive compared to hand crafted 
features. Our main hypothesis is that  the ability of CNN to generate 
hierarchical representations of the features could lead to an 
effective understanding of the relationship between signal patterns 
and the valence of emotions.

We performed an initial empirical study to compare the collected 
emotion classification methods based on an unified way.  Our results 
show that ... ******* mostrar resultados y coclusiones...

The rest of the paper is structured as follows. Section \ref{data} 
explores the publicly  available EEG datasets for emotion 
recognition.  Section  \ref{methods} describes what we consider 
the most representative methods for emotion classification, along 
with a  deep learning alternative. Section \ref{study}
sets a formal way to unify and compare the performance of the 
methods. Section \ref{discussion} discusses the results and the 
limitations of the analysis. Finally, Section \ref{conclusion} outlines 
the conclusions and the future work. 

\section{Data Sources Analysis}
\label{data}

Literature shows several attempts  to provide datasets for EEG-based 
emotion recognition. These approaches differ in many factors, such as  nature 
of the stimuli and number and type of participants. In this section,  a detailed 
summary is provided, followed by a qualitative and quantitative
comparison.  The goal is to highlight the strengths and limitations in order to 
facilitate the selection, given  a specific goal. The following list is based on 
information available online by May 2015.

\subsection{DEAP}
Koelstra et al. \cite{deap2011} released the Database for Emotion Analysis 
using Physiological Signals (DEAP). This dataset contains electroencephalogram 
and peripheral physiological signals of 32 healthy participants (50\% female), 
aged between 19 and 37 (mean age 26.9), while they were watching
40 one-minute long excepts of music videos. For each video, the dataset has 
a label for valence, arousal, dominance and liking levels according a process 
that combined  Last.fm application and subjective annotation of subjects.  The data 
is available include 48 channels (32 EEG channels, 12 peripheral channels, 3 
unused channels and 1 status channel) at a sample rate of 512Hz. Due to
 different revision of the hardware, there are some minor differences in the 
 format, mainly regarding to the order of the  channels.

 \subsection{MAHNOB-HCI}
 In 2012, Soleymani et al. \cite{soleymani2012multimodal} published the MAHNOB-HCI\footnote{http://mahnob-db.eu}, a Multimodal Database for Affect Recognition and Implicit Tagging. Face videos, audio signals, eye gaze data and peripheral/central nervous system physiological signals are available for researchers in those mentioned fields. The characteristics of the database include a total of 27 subjects (11 male and 16 female) with ages between 19 and 40 years old (M=26.06; SD=4.39), and the following recordings: 32-channel EEG (256 Hz); peripheral physiological signals(256 HZ); face and body videos using 6 cameras (60 f/s); eye gaze (60 Hz) and audio (44.1 Hz). This work includes two experiments, the first one was related to the emotional responses to visual stimuli (videos), for which 20 videos were selected for subjects to self-assess using emotional keywords, arousal, valence, dominance and predictability. Experiment 2 was related to implicit tagging and subjects had to report (dis)agreement to the displayed tag for 28 images and 14 videos.
 

\subsection{eNTERFACE'06}
In the context of Enterface 2006, the project Emotion Detection in the Loop from Brain Signals and Facial Images by Savran et al. \cite{enterface06} considered the creation of an affective assessment database\footnote{http://www.enterface.net/results/}. EEG, fNIRS (functional Near Infrared Spectroscopy) and peripheral signals, namely galvanic skin response (GSR), respiration and blood volume pressure were recorded in order to prove the feasibility of a multimodal approach for emotion recognition. Two experiments were conducted for collecting the data. The first one included 5 subjects who were asked to self assess the emotions elicited by diverse IAPS images of three classes (ecalm, exciting positive and exciting negative), while the mentioned responses were being recorded at 1024 Hz. The second experiment considered the display of three kind of emotions, neutral, happiness and disgust. In experiment 2, 16 subjects (10 male and 6 female; M=25 years old), where shown videos from the DaFEx database\footnote{https://i3.fbk.eu/resources/dafex-database-kinetic-facial-expressions} while face video and fNIRS where recorded.




%For our research we used the famous dataset for the analysis of human affective states called
 %Database for Emotion Analysis using Physiological Signals (DEAP)~\cite{deap2011}. This
% dataset contains electroencephalogram and peripheral physiological signals of 32 healthy
 %participants (50\% female), aged between 19 and 37 (mean age 26.9), while they were watching
% 40 one-minute long excepts of music videos. For each video, the dataset has a label for
 %valence, arousal, dominance and liking levels according a process that mixed Last.fm application
 %and subjective annotation of subjects. 

%The web site of the dataset (\url{http://www.eecs.qmul.ac.uk/mmv/datasets/deap/}) allows
 %you to download the raw data of the experiment (bdf files). These files are generated by the Active
 %recording software of Biosemi and incorporate signals from 48 channels (32 EEG channels, 12 peripheral
 %channels, 3 unused channels and 1 status channel) at a sample rate of 512Hz. The experiments were recorded
 %in Twente in The Netherlands and in Geneva in Switzerland, and due to a different revision of the hardware,
 %there are some minor differences in the format, mainly regarding to the order of the channels.
    
%In addition, it is available a pre-processed dataset for Python and MATLAB platforms. This dataset contains
 %files where the data signals is downsampled to 128Hz and the minor differences in the format between both
% experiment locations are resolved. Also, the next steps were applied:
%\begin{enumerate}
%\item A bandpass frecuency filter from 4.0 to 45.0 Hz was applied.
%\item Electrooculogram (EOG) artifacts such as blinks and saccades were removed.
%\item Data was averaged to the common reference.
%\item The data was segmented into 60 second trials and a 3 second pre-trial baseline removed.
%\item The trials were reordered from presentation order to video (Experiment\_id) order.
%\end{enumerate} 

%In this research, due to the convenient pre-processing carried out, we chose to use the pre-processed
 %dataset. For each subject we had the next arrays:

%\begin{table}[h!]
%\footnotesize
%\begin{center}
%\begin{tabular}{|p{1cm}|p{2cm}|p{4.5cm}|}
%      	\hline 
 %     	Array name & Array shape & Array content \\ 
  %    	\hline 
   %   	data & 40 x 40 x 8064 & video/trial x channel x signal \\ 
    %  	\hline
     % 	labels & 40 x 4 & video/trial x label (valence, arousal, dominance, liking) \\ 
     % 	\hline 
%\end{tabular} 
%\end{center}
%\caption{Pre-processed dataset arrays}
%\label{tab:dataset}
%\end{table}       	
	

\section{EEG based Recognition Models}
\label{methods}
	
Based on the work of Jenke et al.\cite{jenke2014feature}, where a 
comprehensive survey on  the features and models for emotion 
classification is presented, we performed a selection of the most 
representative in terms of the treatment of the data. These methods 
heavily rely on manually generated features, which basically represents 
the state of the art in the field \cite{}. We provide a detailed description 
of the steps involved and the replication efforts we carried out. 

Additionally, as it is well known that the performance of any 
classification method depends on the representation of the data 
\cite{bengio2013representation}, 
we propose a way to adapt a Convolutional Neural 
Network approach to classify emotions, in order to compare  
manually crafted and automatic feature selection. 

\subsection{ Multi-Modal Bio-Potential Signals}	

In \cite{takahashi2004}, Takahashi et al. proposed a  method for 
 classification of 5 emotions (joy, anger, sadness, fear, and relax)
using multi-modal biopotential signals such as brain activity, pulse and
 skin conductance. The classification is based on SVM and NN, reaching 
 accuracies of  41.7\%  66.7\% for 5 and 3 emotions, respectively. 
 
As this  study represents one of the first attempts to implement 
machine learning methods on biological signal features for emotion
 recognition, we selected as a baseline for our empirical study. 
 
\subsubsection{Data Collection}

The data collection was performed through a controlled 
experiment using 3 different devices for each type of signal. 
Firstly,  a  brain-computer interface was used for to measure 
the brain activity. This device included 
three dry electrodes that have to be located on the subject's 
forehead. Secondly, a pulseoxymeter was used to capture 
pulse signal. It consists of a sensor clip and an amplifier, which 
is located  on the  subject's earlobe. Lastly, a skin 
conductance meter composed by two electrodes and an amplifier. 
%This device was located on a mouse in order to 
%contact the fingers of the subject when a click is done. 

The experiment  was applied to 12  male subjects and  
consisted of stimulation based  on commercial 
films presented on a screen. These films were previously 
evaluated and manually labeled. This methodology resembles
partially the experimental setting performed for DEAP, such
as the use of video, however, the magnitude and complexity of
the data captured is drastically lower.

%This data collection process shows some similarities with 
%DEAP dataset, however there are some notorious 
%differences. For example, both datasets were generated 
%using audio-visual stimuli, but in the case of DEAP 
%dataset the videos are clips from famous songs and this study used commercial films.
%Another difference is the system used for collecting data. In the Takahashi study were
%used three different devices, and in the particular case of EEG just one channel was the
%output. On the other hand, DEAP dataset have 32 channels for EEG and 8 extra channels
%for other purposes, including skin conductance, but excluding pulse. Finally, the amount of
%subjects each study recorded is significantly different, while this study just 
%recorded 12 subject, DEAP dataset includes 32 of them.                

\subsubsection{Feature Extraction}

To characterize the data this work used the next statistical features:
\begin{enumerate}
\item Mean:  $\mu_{X} = \frac{1}{N}\sum^{N}_{n=1} X(n)$
\item Standard Deviation:  $\sigma_{X} = \sqrt{ \frac{1}{N}\sum^{N}_{n=1} (X(n)-\mu_{X})^{2} }$
\item Mean of absolute differences:  $\delta_{X} = \frac{1}{N-1} \sum^{N-1}_{n=1}|X(n+1)-X(n)|$
\item Mean of normalized absolute differences: $\overline{\delta_{X}} = \frac{\delta_{X}}{\sigma_{X}}$
\item Mean of absolute second differences:  $\gamma_{X} = \frac{1}{N-2} \\
 \sum^{N-2}_{n=1}|X(n+2)-X(n)|$
\item Mean of normalized absolute second differences: $\overline{\gamma_{X}} = \frac{\gamma_{X}}{\sigma_{X}}$
\end{enumerate}  

Once these features were obtained, a feature vector was created according the following 
expression:
\begin{equation}
\begin{split}
x^{T} = [\mu_{e}\ \sigma_{e}\ \delta_{e}\ \overline{\delta_{e}}\ \gamma_{e}\ %\overline{\gamma_{e}}\ 
\mu_{p}\ \sigma_{p}\ \delta_{p}\ \overline{\delta_{p}}\ \gamma_{p}\ %\overline{\gamma_{p}}\ \\
\mu_{s}\ \sigma_{s}\ \delta_{s}\ \overline{\delta_{s}}\ \gamma_{s}\ %\overline{\gamma_{s}}\ ] 
\end{split}
\end{equation}
Where $e$ indicates EEG signals, $p$ pulse, and $s$ skin conductance.

Following the same method of feature extraction for DEAP dataset, 
we could obtain a similar feature vector, however as we mentioned before, 
features related with pulse could not be added. As a result we generate 
a feature vector with 192 columns related to EEG (32 channel x 6 statistical 
features) plus 6 columns related to skin conductance.

\subsubsection{Emotion Recognition method}

This study used two algorithms to classify emotions given bio-potentials signals.
First, a Support Vector Machine (SVM) using a multiclass classification is designed under 
a one-vs-all method, in other words, for each emotion was created a SVM, and the best
classification results indicated the corresponding class. A Gaussian function was used as a kernel function.
Second, an Artificial Neural Network using three layers and trained by Levenberg-Marquart method.
A sigmoid function was used as an activation function of the nodes. As same as the first algorithm,
for each emotion a ANN was created using a similar process to classify.

To evaluate the results a leave-one-out cross-validation method was carried out. 
For the SVM method with 5 emotions the results was 41.7\% and for 3 emotions was 66.7\%.
In the case of ANN method, for 5 emotions the results was 31.7\% and for 3 emotions was
63.9\%.   

To build a similar process using DEAP dataset, we used A SVM algorithm using a
one-vs-all method and a Gaussian function is used as a kernel function. 

\subsubsection{Related Work}

As this work is one of the first in using statistical variables from 
bio-potentials signals over machine learning algorithms, there are other works
that have tried to replicated it. 

Wang et. al. in \cite{wang2011} made a similar experiment
and used the same statistical features for the analysis, but also 
added frequency domain features such as Fast Fourier Transform. His
results are better comparing when frequency domain features are not used.

Liu et. al. in \cite{liu2013}

%\subsection{Murugappan method (2010)}	

%Murugappan et al. have quite an interesting story related to the emotion assessment through EEG analysis. Starting from 2008, several studies could be found in literature corresponding to this author, using different combinations of features, EEG channels and algorithms in order to classify discrete emotions such as disgust, happy, surprise, sad and anger.(CITAR los paper de muru).

%For the present benchmark, we are centered in one of the mentioned Murugappan's works, where an experiment was conducted for collecting EEG data of 20 subjects, for the classification of five emotions, namely disgust, happy, surprise, fear and neutral. A wavelet-based approach was performed, three different features were proposed for the analysis and two classifiers were used, obtaining a maximimum average classification rate of 83.26\% with KNN and 75,21\% with LDA \cite{Murugappan2010Classification}.

%Since Murugappan is an active actor in the emotion recognition field, turns to be important to include this study in benchmark and see how those proposed features behave with the DEAP dataset.

\subsection{Wavelet Decomposition Method}	

Murugappan et al. have quite an interesting story related to the emotion assessment through EEG analysis. Starting from 2008, several studies could be found in literature corresponding to this author, using different combinations of features, EEG channels and algorithms in order to classify discrete emotions such as disgust, happy, surprise, sad and anger \cite{murugappan2008time,murugappan2009wavelet}.

For the present benchmark, we are centered in one of the mentioned Murugappan's works, where an experiment was conducted for collecting EEG data of 20 subjects, for the classification of five emotions, namely disgust, happy, surprise, fear and neutral. A wavelet-based approach was performed, three different features were proposed for the analysis and two classifiers were used, obtaining a maximimum average classification rate of 83.26\% with KNN and 75,21\% with LDA \cite{murugappan2010classification}.

Since Murugappan is an active actor in the emotion recognition field, turns to be important to include this study in benchmark and see how those proposed features behave with the DEAP dataset.

\subsubsection{Data Collection}

The data acquisition process considered a randomly ordered presentation of emotionally selected video clips with different time durations. The protocol followed consisted of presenting natural scene pictures and a ``soothing music'' for several seconds before the experimental session, which made subjects feel calm and mind relaxed. Later, 5 trials for disgust, happy and surprised emotions were followed by 4 trials of fear and neutral emotions. EEG data of 64 channels at a 256 HZ sampling rate were collected with the Nevus EEG device, whose electrodes were placed according to the International 10-10 system. A total number of 20 subjects took the experiment (3 females  and 17 males with ages between 21 and 39 years old).

\subsubsection{Feature Extraction}

Murugappan's approach takes into account the utilization of two sorts of features: Statistical and Wavelet-based. Based in previous section (Takahashi method), we are not performing statistical feature analysis, thus we focus on the proposed wavelet features.

The "db4" wavelet function is used to separate the EEG signals into 5 levels of decomposition, and three frequency bands (alpha, beta and gamma) are considered for deriving the following features:

\begin{enumerate}
\item \textit{Recoursing Energy Efficiency (REE)}, defined as

\begin{equation}
REE_(\gamma-3b) = \frac{E_\gamma}{E_(total-3b)}
\end{equation}
\label{ec:ree}

\item \textit{Logarithmic REE (LREE)}, 

\begin{equation}
LREE = \log(REE)
\end{equation}
\label{ec:lree}

\item \textit{Absolute Logarithmic REE (ALREE)}, 

\begin{equation}
ALREE = \mathopen|LREE\mathclose|
\end{equation}
\label{ec:alree}


\end{enumerate}

Where the total energy of the three bands is:

\begin{equation}
E_(total-3b) = E_\alpha + E_\beta + E_\gamma
\end{equation}

\subsubsection{Emotion Recognition Method}

The process of emotion recognition used by Murugappan consisted in the utilization of two machine learning algorithms, namely K-Nearest Neighbors (KNN) and Linear Discriminant Analysis (LDA), along with a 5-fold cross validation. The same process was applied in this research for classifiying three states of emotion in the valence dimension, such as \emph{positive}, \emph{neutral} and \emph{negative}.

Since the best results obtained by Murugappan were those related with the wavelet energy-based features and the maximum amount of EEG channels (64), we include the same features and our maximum quantity of electrodes (32). We used a 5-Fold Cross Validation method for evaluation.

Contrary to the original results, in our case KNN performed worse than LDA, with accuracies of 41,56\% and 63,28\% respectively with the best behaved feature, ALREE. In \cite{jenke2014feature}, a Quadratic Discriminant Analysis (QDA) classifier is used for emotion recognition with different features, thus for keeping on comparing performances, QDA was applied in the same for, with 70,31\% accuracy for LREE feature as the best result.

\subsubsection{Related}
Wang et al. \cite{wang2014} used diverse features for classifying emotional states from EEG data analysis. With a wavelet-based focus, authors obtained an accuracy of 78.41\% for the wavelet Entropy feature, with a linear SVM model.



\subsection{Convolutional Neural Networks}

The recent renaissance in Artificial Neural Network research 
through the so-called \emph{deep learning}, has led us explore 
the use of these sets of techniques for EEG-based emotion 
recognition. in that sense, we interested in testing if the ability
to automatically learn  hierarchical feature representations can
improve the classification accuracy in this domain. 

As stated in \cite{stober2014using} EEG can be can be modeled 
just a waveform, using a one
dimensional representation, or as a frequency spectrum, using 
two dimensional representation.

\subsection{Architecture}

Filter Layer 

Activation Layer
 
Pooling Layer



\cite{lecun1995convolutional}
\cite{zheng2014time}

\cite{stober2014classifying}
\cite{xing2010brief}

\section{Empirical Study}
\label{study}

In this section, we present an  exploratory 
analysis of the selected approaches based on a standarized
evaluation process. Our main goal is to provide the basis
of a initial benchmark that eventually could be used in future
studies.

\subsection{ Evaluation Design} 

The first step is to define a clear way to compare the 
studied approaches. We are dealing with a multi class classification
problem, where  each subject performed a defined set of trials. As
each trial encapsulates the visualization of a video clip in the DEAP 
repository, we consider
this level as the most appropriate to evaluate. It allows to perform 
further analysis across different dimensions, for example, it is possible
to obtain an aggregated performance score for each  trial (video) across all users. Similarly, it is possible to obtain the average accuracy for one 
user across all trials.

Given the above,  for the specific case of DEAP, it provides 32 subjects, 
each with 40 instances. Therefore, it is expected as output for any classification method in this study, a  40 x 32 matrix  $O$ in  which each element $o_{ij}$ represents the predicted emotion class associated to the instance $i$ for the user $j$.
 
Another relevant aspect is to choose the representation of the class itself. DEAP provides several  values that can be used to 
represent the polarity of an emotion . For simplicity, we chose to used 
the pre-computed valence score  and discretize to three classes, 
encoding the classes from three equal segments (reported scores
range from 1 to 9). 

For the evaluation we adopted the \emph{leave-one-out} cross validation technique (LOOCV).




\subsection{ Results and Analysis}

Given 



\subsection{Remarks}



The code used in this empirical study is available at : 

\section{Discussion}
\label{discussion}

\subsection{Threats to Validity}

The proposed  study has a main goal the formalization
of the analysis into a unified analysis framework 
that could allow researchers to compare and
visualize and improvements in more clear way. 
As this is quite a challenging goals, there are several
threats to the validity of the results. 

\textbf{Internal validity: }  ****hablar sobre la metrica

\textbf{External validity: } In this study, we chose only three approaches
for comparison. 

\textbf{ Construct validity: } 


\subsection{Feasibility}

%\section{Related Work}

\section{Conclusions and  Future Work}
\label{conclusion}

In this paper, we proposed the first step towards the unification 


Our future work is aimed in two directions. Firstly, as a broad
goal, our idea is to continue replicating and  storing EEG-based 
classification methods reported in the literature. With this, we hope
to generate a public repository where the community of researchers
could carry out studies and visualize improvements in  a easier way. 
With this we hope to contribute to field by providing a benchmark
framework for emotion classification.

Secondly, given the promising results obtained by CNN, we are
interested on studying how the hyperparameter setup affects the 
performance of the classification. To this end, we are currently testing
the use of bayesian optimization as well as other unsupervised methods.



%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns


\end{document}
