% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{hyperref}
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

%\title{Benchmarking for emotion recognition in DEAP dataset}
\title{Towards An Unified Analysis Framework for EEG-based Emotion Recognition}


\numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
some people
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.


% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
\vspace{6cm}
\end{abstract}

% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

\keywords{EEG, Machine Learning, Emotion Recognition}

\section{Introduction}
Emotion is critical aspect of the human behavior that plays 
a  significant role in  activities such as communication and 
learning.  Historically, researchers have focused on psycho 
physiological variables, such as posture, language 
(voice intonation), facial expression and gestures to identify 
and classify emotions and relate them to behavior patterns 
 or decision making processes.
The use of electroencephalography (EEG) to study the 
emotion recognition  has allowed the 
support of disciplines such as Human Computer Interaction 
(HCI) and Brain Computer Interfaces (BCI). In this context, 
the vision proposed is that in the future, machines should 
have the ability to identify the emotional state of humans 
in order to provide an effective assistance. This could 
represent a relevant advance in fields such as health care 
and education.

EEG-based emotion recognition usually follows a sequence 
of steps, such as  stimuli selection, feature selection and 
engineering, classification and evaluation. Literature reports 
a high level of heterogeneity on each of these steps, given 
the inherent richness of the data (usually captured through 
a sophisticated configuration of electrodes) , the multimodality 
of the stimuli and the wide range of analytical tools. Although 
this could be seen as a beneficial element, the recent increment
 in proposed approaches makes it difficult to perform a reliable 
comparison to assess the real impact , applicability  and level 
of generalization that the current studies.  Our vision is that, 
based on the collaborative work with other members of the 
community, in the future it will be possible to have a solid 
benchmark that could increase the performance and pace 
of the research in this area, similarly as benchmarks such 
as  \cite{ILSVRC15} and \cite{Judd_2012} have boosted 
the progress in the areas of image recognition and visual 
saliency, respectively. 

In this paper, our  goal is to firstly  study the quality of 
the available datasets for EEG-based recognition, in 
terms of their level of generalization. After choosing 
one, select the current state of the art  in terms of feature
 extraction and try to replicate the their results keeping  
 in mind the unification as a primary goal. 

After setting up a baseline based on the study of related 
work, we explored the use of a new paradigm  based on 
Representation Learning: the use of Convolutional Neural 
Networks (CNN) for  EEG-based emotion recognition.  Our main
motivation is try to generate an alternative to the feature 
engineering process present in EEG-based emotion recognition, 
which usually turns out to be difficult and  excessively laborious. In 
that sense, we are interested in analyzing if an automatic feature 
extraction could be competitive compared to hand crafted 
features. Our main hypothesis is that  the ability of CNN to generate 
hierarchical representations of the features could lead to an 
effective understanding of the relationship between signal patterns 
and the valence of emotions.

We performed an initial empirical study to compare the collected 
emotion classification methods based on an unified way.  Our results 
show that...

The rest of the paper is structured as follows. Section \ref{data} explores 
the publicly  available EEG datasets for emotion recognition.  Section 
\ref{methods} describes what we consider the most representative methods
for emotion classification, along with a  deep learning alternative. Section \ref{study}
sets a formal way to unify and compare the performance of the methods. 
Section \ref{discussion} discusses the results and the limitations of the analysis. Finally,
Section \ref{conclusion} outlines the conclusions and the future work. 

\section{Data Sources}
\label{data}

hablar de las diferentes fuentes. comparar cualitativa y cuantitativa mente.. decir por que deap.



%For our research we used the famous dataset for the analysis of human affective states called
 %Database for Emotion Analysis using Physiological Signals (DEAP)~\cite{deap2011}. This
% dataset contains electroencephalogram and peripheral physiological signals of 32 healthy
 %participants (50\% female), aged between 19 and 37 (mean age 26.9), while they were watching
% 40 one-minute long excepts of music videos. For each video, the dataset has a label for
 %valence, arousal, dominance and liking levels according a process that mixed Last.fm application
 %and subjective annotation of subjects. 

%The web site of the dataset (\url{http://www.eecs.qmul.ac.uk/mmv/datasets/deap/}) allows
 %you to download the raw data of the experiment (bdf files). These files are generated by the Active
 %recording software of Biosemi and incorporate signals from 48 channels (32 EEG channels, 12 peripheral
 %channels, 3 unused channels and 1 status channel) at a sample rate of 512Hz. The experiments were recorded
 %in Twente in The Netherlands and in Geneva in Switzerland, and due to a different revision of the hardware,
 %there are some minor differences in the format, mainly regarding to the order of the channels.
    
%In addition, it is available a pre-processed dataset for Python and MATLAB platforms. This dataset contains
 %files where the data signals is downsampled to 128Hz and the minor differences in the format between both
% experiment locations are resolved. Also, the next steps were applied:
%\begin{enumerate}
%\item A bandpass frecuency filter from 4.0 to 45.0 Hz was applied.
%\item Electrooculogram (EOG) artifacts such as blinks and saccades were removed.
%\item Data was averaged to the common reference.
%\item The data was segmented into 60 second trials and a 3 second pre-trial baseline removed.
%\item The trials were reordered from presentation order to video (Experiment\_id) order.
%\end{enumerate} 

%In this research, due to the convenient pre-processing carried out, we chose to use the pre-processed
 %dataset. For each subject we had the next arrays:

%\begin{table}[h!]
%\footnotesize
%\begin{center}
%\begin{tabular}{|p{1cm}|p{2cm}|p{4.5cm}|}
%      	\hline 
 %     	Array name & Array shape & Array content \\ 
  %    	\hline 
   %   	data & 40 x 40 x 8064 & video/trial x channel x signal \\ 
    %  	\hline
     % 	labels & 40 x 4 & video/trial x label (valence, arousal, dominance, liking) \\ 
     % 	\hline 
%\end{tabular} 
%\end{center}
%\caption{Pre-processed dataset arrays}
%\label{tab:dataset}
%\end{table}       	
	

\section{EEG based Recognition Models}
	
\subsection{Emotion Recognition from Multi-Modal Bio-Potential Signals (2004)}	

In 2004, Kazuhiko Takahashi published his work called 
\textit{Remarks on Emotion Recognition from Multi-Modal Bio-Potential Signals}~\cite{takahashi2004}. This 
research aimed to achieve a classification of 5 emotions (joy, anger, sadness, fear, and relax)
using multi-modal biopotential signals such as brain signals, pulse and skin conductance. He
 did experiments in order to collect data and then make a posterior analysis using algorithms
 like Support Vector Machine and Artificial Neural Networks. His results reached a precision of
 41.7\% for 5 emotions and 66.7\% for 3 emotions. 

This work is one of the first that tries to use machine learning algorithms
 over biological signals features for emotion recognition. In order to see
 how this field has been growing over last decade, this research is a good
 candidate to start the list of included works in the benchmark.  

\subsubsection{Data Collection}

The data collection was done through an experiment using 3 different devices for each kind of signal. 
First, to measure the brain signals it was used a simple brain-computer interface
(The Cyberlink Interface, Brain Actuated Technologies, Inc.). This device includes 
three dry electrodes that have to be located on subject's forehead. Second, to record 
pulse signal was used a pulseoxymeter, composed by a sensor clip and an amplifier, that 
was located on subject's earlobe. Third, a skin conductance meter composed by
two electrodes and an amplifier. This device was located on a mouse in order to 
contact the fingers of the subject when a click is done. 

The experiment was applied to 12 subjects (male, native Japanese, age 21-25). This
procedure aimed to record bio-potentials signals while subjects were stimulated showing commercial 
films that were broadcasted on TV. The commercial films were previously evaluated by 
human subjects who did not participate in the experiment. Each films should stimulate a 
emotion according to the previous evaluation.         

\subsubsection{Feature Extraction}

To characterize the data this work used the next statistical features:
\begin{enumerate}
\item Mean 
\begin{equation}
\mu_{X} = \frac{1}{N}\sum^{N}_{n=1} X(n)
\end{equation}
\item Standard Deviation 
\begin{equation}
\sigma_{X} = \sqrt{ \frac{1}{N}\sum^{N}_{n=1} (X(n)-\mu_{X})^{2} }
\end{equation}
\item Mean of absolute differences 
\begin{equation}
\delta_{X} = \frac{1}{N-1} \sum^{N-1}_{n=1}|X(n+1)-X(n)|
\end{equation}
\item Mean of normalized absolute differences
\begin{equation}
\overline{\delta_{X}} = \frac{\delta_{X}}{\sigma_{X}}
\end{equation}
\item Mean of absolute second differences  
\begin{equation}
\gamma_{X} = \frac{1}{N-2} \sum^{N-2}_{n=1}|X(n+2)-X(n)|
\end{equation}
\item Mean of normalized absolute second differences
\begin{equation}
\overline{\gamma_{X}} = \frac{\gamma_{X}}{\sigma_{X}}
\end{equation}
\end{enumerate}  

Once these features were obtained, a feature vector was created according the following 
expression:
\begin{equation}
\begin{split}
x^{T} = [\mu_{e}\ \sigma_{e}\ \delta_{e}\ \overline{\delta_{e}}\ \gamma_{e}\ \overline{\gamma_{e}}\ 
\mu_{p}\ \sigma_{p}\ \delta_{p}\ \overline{\delta_{p}}\ \gamma_{p}\ \overline{\gamma_{p}}\ \\
\mu_{s}\ \sigma_{s}\ \delta_{s}\ \overline{\delta_{s}}\ \gamma_{s}\ \overline{\gamma_{s}}\ ] 
\end{split}
\end{equation}
Where $e$ indicates EEG signals, $p$ pulse, and $s$ skin conductance.

\subsubsection{Emotion Recognition method}

This study used two algorithms to classify emotions given bio-potentials signals.
First, a Support Vector Machine (SVM) using a multiclass classification is designed under 
a one-vs-all method, in other words, for each emotion was created a SVM, and the best
classification results indicated the corresponding class. A Gaussian function was used as a kernel function.
Second, an Artificial Neural Network using three layers and trained by Levenberg-Marquart method.
A sigmoid function was used as an activation function of the nodes. As same as the first algorithm,
for each emotion a ANN was created using a similar process to classify.

To evaluate the results a leave-one-out cross-validation method was carried out. 
For the SVM method with 5 emotions the results was 41.7\% and for 3 emotions was 66.7\%.
In the case of ANN method, for 5 emotions the results was 31.7\% and for 3 emotions was
63.9\%.   


\subsection{Murugappan method (2010)}	

Murugappan et al. have quite an interesting story related to the emotion assessment through EEG analysis. Starting from 2008, several studies could be found in literature corresponding to this author, using different combinations of features, EEG channels and algorithms in order to classify discrete emotions such as disgust, happy, surprise, sad and anger.(CITAR los paper de muru).

For the present benchmark, we are centered in one of the mentioned Murugappan's works, where an experiment was conducted for collecting EEG data of 20 subjects, for the classification of five emotions, namely disgust, happy, surprise, fear and neutral. A wavelet-based approach was performed, three different features were proposed for the analysis and two classifiers were used, obtaining a maximimum average classification rate of 83.26\% with KNN and 75,21\% with LDA \cite{Murugappan2010Classification}.

Since Murugappan is an active actor in the emotion recognition field, turns to be important to include this study in benchmark and see how those proposed features behave with the DEAP dataset.

\subsection{Convolutional Neural Networks}

\cite{lecun1995convolutional}
\cite{zheng2014time}
\cite{stober2014using}
\cite{stober2014classifying}
\cite{xing2010brief}

\section{Empirical Study}

\subsection{How to compare} 

\subsection{metric} 

\subsection{evaluation Steps}

\subsection{remarks}


\section{Discussion}

\subsection{threats to validity}

\section{Related Work}

\section{Conclusions}


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns


\end{document}
